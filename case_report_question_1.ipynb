{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Index\n",
    "1. Index\n",
    "2. Problem definition\n",
    "3. Solution strategy\n",
    "4. Design choies\n",
    "5. Implementation approach\n",
    "6. Implementation\n",
    "7. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reshaped holds six discovery meetings weekly and prioritizes gaining a deep understanding of potential clients beforehand.  \n",
    "- A key part of this preparation involves reviewing annual reports,  \n",
    "  - A task that is often time-consuming due to their length and inconsistent formatting.  \n",
    "- To streamline this process, we aim to leverage a **Large Language Model (LLM)**.  \n",
    "- The LLM should be capable of **answering questions based on the content of the annual reports**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solution Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language models often lack up-to-date or document-specific knowledge.  \n",
    "  - As a result, they cannot reliably analyze arbitrary documents on their own.\n",
    "- To address this limitation, a language model can be combined with an external information source.  \n",
    "  - This approach is known as **Retrieval Augmented Generation (RAG)**.\n",
    "- In RAG, the model retrieves real documents to **\"ground\"** its answers in factual content.  \n",
    "- This enables more **accurate, reliable, and current responses**, especially for niche or rapidly changing topics.  \n",
    "- RAG is ideal for use cases like **chatbots, search assistants,** and **enterprise knowledge tools**.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "A typical RAG process consists of the following steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](docs/img/flowchart_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every step in the RAG process described above involves making choices. I will go over the main choices and highlight possible alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Document Loading\n",
    "\n",
    "- Many libraries are capable of extracting text from a PDF file (e.g., `PyPDF2`, `Tika`, LangChain's `PyPDFLoader`).  \n",
    "- The main decision is not which library to use, but what **form** the output should take:  \n",
    "  - Flat text  \n",
    "  - Markdown  \n",
    "- In many cases, LLMs perform better when the input is **structured using Markdown**.  \n",
    "- However, extracting precise and consistent Markdown formatting is difficult.  \n",
    "- The results of the **splitting** step are highly sensitive to slight perturbations in Markdown.  \n",
    "- For this reason, we chose a more robust approach: loading documents as **flat text**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Splitting\n",
    "\n",
    "- Text is split before being stored in a **vector database**  \n",
    "  - This ensures that entries are small and focused enough for **accurate search and efficient retrieval**.\n",
    "\n",
    "- I evaluated several splitters from LangChain:  \n",
    "  - `CharacterTextSplitter`  \n",
    "  - `RecursiveCharacterTextSplitter`  \n",
    "  - `MarkdownHeaderTextSplitter`\n",
    "\n",
    "- The Markdown splitter is compelling because it preserves metadata,  \n",
    "  - but it's highly sensitive to changes in Markdown formatting.\n",
    "\n",
    "- `RecursiveCharacterTextSplitter` offers slightly more flexibility than `CharacterTextSplitter`.\n",
    "\n",
    "- Key parameters:  \n",
    "  - `Chunk size`  \n",
    "  - `Overlap`\n",
    "\n",
    "- Based on average paragraph length, I chose:  \n",
    "  - `chunk_size = 500`  \n",
    "  - `overlap = 100`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Storage\n",
    "\n",
    "- After splitting and embedding the text, vectors are stored in a vector database:  \n",
    "  - This enables fast similarity search during retrieval.\n",
    "\n",
    "- Common vector stores include:  \n",
    "  - **FAISS**  \n",
    "    - Open source, fast for local projects  \n",
    "    - Not scalable  \n",
    "  - **Chroma**  \n",
    "    - Open source, fast for local projects  \n",
    "    - Not scalable  \n",
    "  - **Pinecone**  \n",
    "    - Offers a paid version, very scalable  \n",
    "    - Can be more difficult to set up\n",
    "\n",
    "- For small projects (like this one), there's very little difference between FAISS and Chroma:  \n",
    "  - Chroma is slightly easier to set up and persist.\n",
    "\n",
    "- **Embeddings** determine how text is converted into vector representations and placed in the vector space:  \n",
    "  - Hundreds of embedding options are available.  \n",
    "  - For ease of use, I worked with **OpenAI embeddings**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Retrieval\n",
    "\n",
    "- Retrieval involves searching a vector database to find the most relevant text chunks:  \n",
    "  - Based on similarity to the query  \n",
    "\n",
    "- Common retrieval methods include:  \n",
    "  - **K-nearest neighbors (KNN):** Find the K chunks most similar to the query  \n",
    "  - **(Self) Filtered search:** Combine KNN with metadata-based search  \n",
    "  - **Maximal Marginal Relevance (MMR):** Maximize variance of responses within the most similar chunks\n",
    "\n",
    "- Characteristics of chunks:  \n",
    "  - No relevant metadata is present  \n",
    "  - Much of the text is relatively similar\n",
    "\n",
    "- Work with the **MMR** retrieval method.\n",
    "  - `Lambda` parameter balances relevance/penalize similarity\n",
    "  - Set `Lambda` = 0.5\n",
    "\n",
    "- The number of chunks retrieved depends on the size of the context window:  \n",
    "  - More chunks = more information  \n",
    "  - More chunks = more noise\n",
    "\n",
    "- The context window for `GPT-3.5 Turbo` is **±16,000** tokens (~12,000 words):  \n",
    "  - Questions 1 and 2 cover a lot of information  \n",
    "  - Number of chunks retrieved = 30\n",
    "\n",
    "N.B.\n",
    "- Newer models (e.g. `GPT-4o` or `GPT-4.1`) have context windows or 100K - 1M\n",
    "  - This could remove the need for chunking/retrieval on single documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Output\n",
    "\n",
    "- To obtain output, the question and retrieved information must be combined.\n",
    "\n",
    "- There are various strategies for combining retrieved information, such as:  \n",
    "  - **Stuffing:** All retrieved chunks are included in the prompt.  \n",
    "  - **Map Reduce:** Each chunk is used separately to answer the question and produce intermediate results.  \n",
    "  - **Refine:** Generates an answer from one chunk and then iteratively refines it with additional chunks.\n",
    "\n",
    "- We’re working with one relatively short document:  \n",
    "  - 23,000 words (± 30,000 tokens)\n",
    "\n",
    "- No need for advanced combination strategies in this case:  \n",
    "  - Opt for **stuffing**.\n",
    "\n",
    "- The system prompt should guide behavior by specifying:  \n",
    "  - Source usage  \n",
    "  - Tone/response style  \n",
    "  - How to handle conflicting information  \n",
    "  - Task type (e.g., Q&A vs. summarization)\n",
    "\n",
    "- The **temperature** parameter controls the randomness of the language model's output:  \n",
    "  - Lower values make responses more focused and deterministic.  \n",
    "  - Higher values increase creativity and variability.\n",
    "\n",
    "- To ensure we stick to facts, we set the temperature to **0**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](docs/img/q1_system_prompt.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 design overview\n",
    "- Document loading = LangChain `PyPDFLoader`\n",
    "- Splitting = LangChain `RecursiveCharacterTextSplitter` (default separators), `chunk size` = 500, `overlap` = 100\n",
    "- Storage = Chroma vector database (OpenAI embeddings)\n",
    "- Retrieval = Maximum marginal relevance (90 chunks fetched, 30 retrieved, `lambda` = 0.5)\n",
    "- Output = Combine question and retrieved information through LangChain Prompt template using stuffing, set temperature to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementation Approach\n",
    "\n",
    "- The implementation of the design was developed by:  \n",
    "  - Iteratively exploring design choices using **ChatGPT**  \n",
    "  - Experimenting with design choices using code snippets implemented by **Cursor**  \n",
    "  - Implementing the folder structure, `utils.py`, and `README` with **Cursor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import importlib\n",
    "import os\n",
    "import utils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Reload utils to ensure changes in functions carry over from helper file\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Specify question 1\n",
    "QUESTION_1 = \"What company is described in the document and what is their business model?\"\n",
    "\n",
    "# Specify question 2\n",
    "QUESTION_2 = \"What are the main risks this company is facing?\"\n",
    "\n",
    "# Specify document locations\n",
    "PDF_PATH = r\"Microsoft_2023_Trimmed.pdf\"\n",
    "\n",
    "# Specify vector database storage directory\n",
    "DB_PERSISTANCE_PATH = r\"db\\annual_report_vector_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load and split PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# Load and split\n",
    "chunks = utils.load_pdf_with_recursive_splitter(PDF_PATH, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Inspect result\n",
    "print(f\"Created {len(chunks)} chunks from the PDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Create and store vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store database\n",
    "vector_db = utils.create_vector_db(chunks=chunks, persist_directory=DB_PERSISTANCE_PATH)\n",
    "\n",
    "# Verify outcome\n",
    "print(\"Vector database created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "NUM_CHUNKS = 30\n",
    "\n",
    "# Retrieve relevant chunks for question 1\n",
    "relevant_chunks_q1 = utils.retrieve_relevant_chunks(vector_db, query=QUESTION_1, k=NUM_CHUNKS)\n",
    "\n",
    "# Retrieve relevant chunks for question 2\n",
    "relevant_chunks_q2 = utils.retrieve_relevant_chunks(vector_db, query=QUESTION_2, k=NUM_CHUNKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Create prompt template and query LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "TEMP = 0\n",
    "\n",
    "# Create template\n",
    "prompt_q1 = utils.setup_prompt(relevant_chunks_q1)\n",
    "\n",
    "prompt_q2 = utils.setup_prompt(relevant_chunks_q2)\n",
    "\n",
    "# Pass prompt for question 1 to LLM\n",
    "answer_q1 = utils.query_llm(system_prompt=prompt_q1, question=QUESTION_1, temperature=TEMP)\n",
    "\n",
    "# Pass prompt for question 2 to LLM\n",
    "answer_q2 = utils.query_llm(system_prompt=prompt_q2, question=QUESTION_2, temperature=TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [(QUESTION_1, answer_q1), (QUESTION_2, answer_q2)]\n",
    "\n",
    "for question, answer in questions:\n",
    "    print(question)\n",
    "    print(answer)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv old specs",
   "language": "python",
   "name": "venv_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
